{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.0.1\n",
      "Torchvision Version:  0.2.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import torch.utils.data as utils\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.optim import lr_scheduler\n",
    "from PIL import Image\n",
    "import PIL.ImageOps    \n",
    "import torch.nn.functional as F\n",
    "import pandas as pd \n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:\n",
    "                img, label = data\n",
    "                img ,label = img.to(device) , label.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    res = model(img)\n",
    "                    res = res.to(device)\n",
    "                    label=label.float()\n",
    "                    label=label.flatten()\n",
    "                    loss = criterion(res, label)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                if torch.abs(torch.sum(res.data - label.data))/128 < 10.0:\n",
    "                    running_corrects += 1\n",
    "                    \n",
    "#                 print(\"model:\", end=''); print(torch.mean(res.data))\n",
    "#                 print(\"loss: \", end=''); print(torch.abs(torch.sum(res.data - label.data))/128)\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepressingNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super(DepressingNet, self).__init__()\n",
    "        self.model = model\n",
    "        self.output_layer = nn.Sequential(nn.Linear(50,1))\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.model(inp)\n",
    "        res = self.output_layer(out)\n",
    "#         res = torch.clamp(out,min=11.0, max=50.0)\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepressingNetDataset():\n",
    "    \n",
    "    def __init__(self,training_csv=None,training_dir=None,transform=None):\n",
    "        self.training_df=pd.read_csv(training_csv)\n",
    "        self.training_df.dropna(inplace=True)\n",
    "        self.training_dir = training_dir    \n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        image_path=os.path.join(self.training_dir,str(self.training_df['image'].iloc[index])+'.jpg')  \n",
    "        img = Image.open(image_path)       \n",
    "        # Apply image transformations\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, torch.from_numpy(np.array([float(self.training_df['mean_rank'].iloc[index])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.show()\n",
    "\n",
    "class Config():\n",
    "    training_csv = \"/home/bill/urban_planning/ranked_depressing_train.csv\"\n",
    "    testing_csv =\"/home/bill/urban_planning/ranked_depressing_test.csv\"\n",
    "    image_dir=\"/home/bill/urban_planning/images_only/\"\n",
    "    train_batch_size = 32\n",
    "    train_number_epochs = 101\n",
    "    num_classes=50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "config = Config()\n",
    "model_name = \"vgg\"\n",
    "num_classes = config.num_classes\n",
    "batch_size = config.train_batch_size\n",
    "num_epochs = config.train_number_epochs\n",
    "feature_extract = True\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DepressingNetDataset(config.training_csv,config.image_dir,\n",
    "                                        transform=transforms.Compose([transforms.Resize((input_size, input_size)),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                                                      ])\n",
    "                                       )\n",
    "\n",
    "test_dataset = DepressingNetDataset(config.testing_csv,config.image_dir,\n",
    "                                        transform=transforms.Compose([transforms.Resize((input_size, input_size)),\n",
    "                                        transforms.CenterCrop(input_size),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                                                      ])\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=8,\n",
    "                        batch_size=config.train_batch_size)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             num_workers=6,\n",
    "                             batch_size=config.train_batch_size,\n",
    "                             shuffle=True)\n",
    "\n",
    "dataloaders_dict={'train':train_dataloader, 'val':test_dataloader}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "model = DepressingNet(model_ft)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      "----------\n",
      "train Loss: 0.5409 Acc: 0.0260\n",
      "val Loss: 0.4069 Acc: 0.0140\n",
      "Epoch 1/100\n",
      "----------\n",
      "train Loss: 0.4391 Acc: 0.0268\n",
      "val Loss: 0.3596 Acc: 0.0276\n",
      "Epoch 2/100\n",
      "----------\n",
      "train Loss: 0.4082 Acc: 0.0269\n",
      "val Loss: 0.4175 Acc: 0.0062\n",
      "Epoch 3/100\n",
      "----------\n",
      "train Loss: 0.3797 Acc: 0.0280\n",
      "val Loss: 0.3277 Acc: 0.0282\n",
      "Epoch 4/100\n",
      "----------\n",
      "train Loss: 0.3628 Acc: 0.0279\n",
      "val Loss: 0.3245 Acc: 0.0268\n",
      "Epoch 5/100\n",
      "----------\n",
      "train Loss: 0.3492 Acc: 0.0277\n",
      "val Loss: 0.3430 Acc: 0.0185\n",
      "Epoch 6/100\n",
      "----------\n",
      "train Loss: 0.3369 Acc: 0.0283\n",
      "val Loss: 0.3532 Acc: 0.0142\n",
      "Epoch 7/100\n",
      "----------\n",
      "train Loss: 0.3268 Acc: 0.0287\n",
      "val Loss: 0.3161 Acc: 0.0248\n",
      "Epoch 8/100\n",
      "----------\n",
      "train Loss: 0.3210 Acc: 0.0283\n",
      "val Loss: 0.3016 Acc: 0.0297\n",
      "Epoch 9/100\n",
      "----------\n",
      "train Loss: 0.3163 Acc: 0.0284\n",
      "val Loss: 0.2997 Acc: 0.0296\n",
      "Epoch 10/100\n",
      "----------\n",
      "train Loss: 0.3113 Acc: 0.0287\n",
      "val Loss: 0.2959 Acc: 0.0300\n",
      "Epoch 11/100\n",
      "----------\n",
      "train Loss: 0.3092 Acc: 0.0282\n",
      "val Loss: 0.3199 Acc: 0.0221\n",
      "Epoch 12/100\n",
      "----------\n",
      "train Loss: 0.3059 Acc: 0.0286\n",
      "val Loss: 0.2951 Acc: 0.0297\n",
      "Epoch 13/100\n",
      "----------\n",
      "train Loss: 0.3019 Acc: 0.0291\n",
      "val Loss: 0.3504 Acc: 0.0115\n",
      "Epoch 14/100\n",
      "----------\n",
      "train Loss: 0.3022 Acc: 0.0287\n",
      "val Loss: 0.3530 Acc: 0.0110\n",
      "Epoch 15/100\n",
      "----------\n",
      "train Loss: 0.3007 Acc: 0.0286\n",
      "val Loss: 0.2948 Acc: 0.0292\n",
      "Epoch 16/100\n",
      "----------\n",
      "train Loss: 0.3011 Acc: 0.0284\n",
      "val Loss: 0.2943 Acc: 0.0287\n",
      "Epoch 17/100\n",
      "----------\n",
      "train Loss: 0.2994 Acc: 0.0284\n",
      "val Loss: 0.2946 Acc: 0.0297\n",
      "Epoch 18/100\n",
      "----------\n",
      "train Loss: 0.2996 Acc: 0.0284\n",
      "val Loss: 0.3125 Acc: 0.0226\n",
      "Epoch 19/100\n",
      "----------\n",
      "train Loss: 0.2988 Acc: 0.0287\n",
      "val Loss: 0.2941 Acc: 0.0294\n",
      "Epoch 20/100\n",
      "----------\n",
      "train Loss: 0.2965 Acc: 0.0291\n",
      "val Loss: 0.2939 Acc: 0.0283\n",
      "Epoch 21/100\n",
      "----------\n",
      "train Loss: 0.2990 Acc: 0.0281\n",
      "val Loss: 0.2872 Acc: 0.0306\n",
      "Epoch 22/100\n",
      "----------\n",
      "train Loss: 0.2973 Acc: 0.0288\n",
      "val Loss: 0.3050 Acc: 0.0259\n",
      "Epoch 23/100\n",
      "----------\n",
      "train Loss: 0.2976 Acc: 0.0285\n",
      "val Loss: 0.2917 Acc: 0.0296\n",
      "Epoch 24/100\n",
      "----------\n",
      "train Loss: 0.2958 Acc: 0.0292\n",
      "val Loss: 0.3122 Acc: 0.0233\n",
      "Epoch 25/100\n",
      "----------\n",
      "train Loss: 0.2960 Acc: 0.0289\n",
      "val Loss: 0.2874 Acc: 0.0308\n",
      "Epoch 26/100\n",
      "----------\n",
      "train Loss: 0.2989 Acc: 0.0280\n",
      "val Loss: 0.2880 Acc: 0.0307\n",
      "Epoch 27/100\n",
      "----------\n",
      "train Loss: 0.2971 Acc: 0.0287\n",
      "val Loss: 0.2887 Acc: 0.0303\n",
      "Epoch 28/100\n",
      "----------\n",
      "train Loss: 0.2966 Acc: 0.0288\n",
      "val Loss: 0.2884 Acc: 0.0302\n",
      "Epoch 29/100\n",
      "----------\n",
      "train Loss: 0.2976 Acc: 0.0287\n",
      "val Loss: 0.2868 Acc: 0.0304\n",
      "Epoch 30/100\n",
      "----------\n",
      "train Loss: 0.2987 Acc: 0.0279\n",
      "val Loss: 0.2896 Acc: 0.0299\n",
      "Epoch 31/100\n",
      "----------\n",
      "train Loss: 0.2954 Acc: 0.0287\n",
      "val Loss: 0.2870 Acc: 0.0304\n",
      "Epoch 32/100\n",
      "----------\n",
      "train Loss: 0.2962 Acc: 0.0286\n",
      "val Loss: 0.3014 Acc: 0.0269\n",
      "Epoch 33/100\n",
      "----------\n",
      "train Loss: 0.2969 Acc: 0.0285\n",
      "val Loss: 0.2880 Acc: 0.0303\n",
      "Epoch 34/100\n",
      "----------\n",
      "train Loss: 0.2957 Acc: 0.0292\n",
      "val Loss: 0.2968 Acc: 0.0285\n",
      "Epoch 35/100\n",
      "----------\n",
      "train Loss: 0.2956 Acc: 0.0291\n",
      "val Loss: 0.2872 Acc: 0.0306\n",
      "Epoch 36/100\n",
      "----------\n",
      "train Loss: 0.2973 Acc: 0.0283\n",
      "val Loss: 0.2906 Acc: 0.0293\n",
      "Epoch 37/100\n",
      "----------\n",
      "train Loss: 0.2960 Acc: 0.0289\n",
      "val Loss: 0.2907 Acc: 0.0296\n",
      "Epoch 38/100\n",
      "----------\n",
      "train Loss: 0.2979 Acc: 0.0283\n",
      "val Loss: 0.2908 Acc: 0.0300\n",
      "Epoch 39/100\n",
      "----------\n",
      "train Loss: 0.2971 Acc: 0.0286\n",
      "val Loss: 0.2884 Acc: 0.0303\n",
      "Epoch 40/100\n",
      "----------\n",
      "train Loss: 0.2955 Acc: 0.0290\n",
      "val Loss: 0.2872 Acc: 0.0308\n",
      "Epoch 41/100\n",
      "----------\n",
      "train Loss: 0.2964 Acc: 0.0288\n",
      "val Loss: 0.3241 Acc: 0.0198\n",
      "Epoch 42/100\n",
      "----------\n",
      "train Loss: 0.2959 Acc: 0.0292\n",
      "val Loss: 0.2871 Acc: 0.0304\n",
      "Epoch 43/100\n",
      "----------\n",
      "train Loss: 0.2969 Acc: 0.0288\n",
      "val Loss: 0.2877 Acc: 0.0304\n",
      "Epoch 44/100\n",
      "----------\n",
      "train Loss: 0.2962 Acc: 0.0290\n",
      "val Loss: 0.2901 Acc: 0.0300\n",
      "Epoch 45/100\n",
      "----------\n",
      "train Loss: 0.2981 Acc: 0.0283\n",
      "val Loss: 0.2893 Acc: 0.0306\n",
      "Epoch 46/100\n",
      "----------\n",
      "train Loss: 0.2958 Acc: 0.0291\n",
      "val Loss: 0.2893 Acc: 0.0305\n",
      "Epoch 47/100\n",
      "----------\n",
      "train Loss: 0.2966 Acc: 0.0287\n",
      "val Loss: 0.2934 Acc: 0.0294\n",
      "Epoch 48/100\n",
      "----------\n",
      "train Loss: 0.2967 Acc: 0.0286\n",
      "val Loss: 0.2913 Acc: 0.0292\n",
      "Epoch 49/100\n",
      "----------\n",
      "train Loss: 0.2959 Acc: 0.0290\n",
      "val Loss: 0.3113 Acc: 0.0237\n",
      "Epoch 50/100\n",
      "----------\n",
      "train Loss: 0.2970 Acc: 0.0284\n",
      "val Loss: 0.2910 Acc: 0.0299\n",
      "Epoch 51/100\n",
      "----------\n",
      "train Loss: 0.2959 Acc: 0.0289\n",
      "val Loss: 0.2981 Acc: 0.0276\n",
      "Epoch 52/100\n",
      "----------\n",
      "train Loss: 0.2960 Acc: 0.0290\n",
      "val Loss: 0.2876 Acc: 0.0306\n",
      "Epoch 53/100\n",
      "----------\n",
      "train Loss: 0.2975 Acc: 0.0284\n",
      "val Loss: 0.2867 Acc: 0.0310\n",
      "Epoch 54/100\n",
      "----------\n",
      "train Loss: 0.2970 Acc: 0.0287\n",
      "val Loss: 0.2887 Acc: 0.0304\n",
      "Epoch 55/100\n",
      "----------\n",
      "train Loss: 0.2961 Acc: 0.0291\n",
      "val Loss: 0.2886 Acc: 0.0307\n",
      "Epoch 56/100\n",
      "----------\n",
      "train Loss: 0.2970 Acc: 0.0286\n",
      "val Loss: 0.2890 Acc: 0.0301\n",
      "Epoch 57/100\n",
      "----------\n",
      "train Loss: 0.2965 Acc: 0.0289\n",
      "val Loss: 0.2924 Acc: 0.0294\n",
      "Epoch 58/100\n",
      "----------\n",
      "train Loss: 0.2967 Acc: 0.0284\n",
      "val Loss: 0.2882 Acc: 0.0308\n",
      "Epoch 59/100\n",
      "----------\n",
      "train Loss: 0.2970 Acc: 0.0287\n",
      "val Loss: 0.2884 Acc: 0.0305\n",
      "Epoch 60/100\n",
      "----------\n",
      "train Loss: 0.2974 Acc: 0.0284\n",
      "val Loss: 0.2875 Acc: 0.0306\n",
      "Epoch 61/100\n",
      "----------\n",
      "train Loss: 0.2963 Acc: 0.0287\n",
      "val Loss: 0.2959 Acc: 0.0287\n",
      "Epoch 62/100\n",
      "----------\n",
      "train Loss: 0.2957 Acc: 0.0291\n",
      "val Loss: 0.2870 Acc: 0.0310\n",
      "Epoch 63/100\n",
      "----------\n",
      "train Loss: 0.2955 Acc: 0.0289\n",
      "val Loss: 0.2866 Acc: 0.0306\n",
      "Epoch 64/100\n",
      "----------\n",
      "train Loss: 0.2977 Acc: 0.0285\n",
      "val Loss: 0.2867 Acc: 0.0306\n",
      "Epoch 65/100\n",
      "----------\n",
      "train Loss: 0.2963 Acc: 0.0288\n",
      "val Loss: 0.2867 Acc: 0.0305\n",
      "Epoch 66/100\n",
      "----------\n",
      "train Loss: 0.2978 Acc: 0.0282\n",
      "val Loss: 0.2989 Acc: 0.0278\n",
      "Epoch 67/100\n",
      "----------\n",
      "train Loss: 0.2965 Acc: 0.0286\n",
      "val Loss: 0.2964 Acc: 0.0290\n",
      "Epoch 68/100\n",
      "----------\n",
      "train Loss: 0.2963 Acc: 0.0285\n",
      "val Loss: 0.2866 Acc: 0.0308\n",
      "Epoch 69/100\n",
      "----------\n",
      "train Loss: 0.2975 Acc: 0.0288\n",
      "val Loss: 0.2868 Acc: 0.0304\n",
      "Epoch 70/100\n",
      "----------\n",
      "train Loss: 0.2970 Acc: 0.0286\n",
      "val Loss: 0.2868 Acc: 0.0306\n",
      "Epoch 71/100\n",
      "----------\n",
      "train Loss: 0.2966 Acc: 0.0285\n",
      "val Loss: 0.3163 Acc: 0.0218\n",
      "Epoch 72/100\n",
      "----------\n",
      "train Loss: 0.2976 Acc: 0.0282\n",
      "val Loss: 0.3146 Acc: 0.0229\n",
      "Epoch 73/100\n",
      "----------\n",
      "train Loss: 0.2977 Acc: 0.0285\n",
      "val Loss: 0.3008 Acc: 0.0273\n",
      "Epoch 74/100\n",
      "----------\n",
      "train Loss: 0.2986 Acc: 0.0283\n",
      "val Loss: 0.3458 Acc: 0.0132\n",
      "Epoch 75/100\n",
      "----------\n",
      "train Loss: 0.2985 Acc: 0.0282\n",
      "val Loss: 0.2872 Acc: 0.0301\n",
      "Epoch 76/100\n",
      "----------\n",
      "train Loss: 0.2963 Acc: 0.0287\n",
      "val Loss: 0.2979 Acc: 0.0285\n",
      "Epoch 77/100\n",
      "----------\n",
      "train Loss: 0.2969 Acc: 0.0285\n",
      "val Loss: 0.3118 Acc: 0.0237\n",
      "Epoch 78/100\n",
      "----------\n",
      "train Loss: 0.2960 Acc: 0.0289\n",
      "val Loss: 0.2874 Acc: 0.0306\n",
      "Epoch 79/100\n",
      "----------\n",
      "train Loss: 0.2962 Acc: 0.0289\n",
      "val Loss: 0.3316 Acc: 0.0178\n",
      "Epoch 80/100\n",
      "----------\n",
      "train Loss: 0.2970 Acc: 0.0286\n",
      "val Loss: 0.2939 Acc: 0.0290\n",
      "Epoch 81/100\n",
      "----------\n",
      "train Loss: 0.2994 Acc: 0.0276\n",
      "val Loss: 0.3080 Acc: 0.0251\n",
      "Epoch 82/100\n",
      "----------\n",
      "train Loss: 0.2965 Acc: 0.0287\n",
      "val Loss: 0.2875 Acc: 0.0304\n",
      "Epoch 83/100\n",
      "----------\n",
      "train Loss: 0.2963 Acc: 0.0288\n",
      "val Loss: 0.2866 Acc: 0.0308\n",
      "Epoch 84/100\n",
      "----------\n",
      "train Loss: 0.2963 Acc: 0.0290\n",
      "val Loss: 0.2868 Acc: 0.0306\n",
      "Epoch 85/100\n",
      "----------\n",
      "train Loss: 0.2970 Acc: 0.0287\n",
      "val Loss: 0.2918 Acc: 0.0293\n",
      "Epoch 86/100\n",
      "----------\n",
      "train Loss: 0.2966 Acc: 0.0287\n",
      "val Loss: 0.2877 Acc: 0.0304\n",
      "Epoch 87/100\n",
      "----------\n",
      "train Loss: 0.2950 Acc: 0.0293\n",
      "val Loss: 0.2877 Acc: 0.0307\n",
      "Epoch 88/100\n",
      "----------\n",
      "train Loss: 0.2965 Acc: 0.0286\n",
      "val Loss: 0.2875 Acc: 0.0306\n",
      "Epoch 89/100\n",
      "----------\n",
      "train Loss: 0.2967 Acc: 0.0286\n",
      "val Loss: 0.2883 Acc: 0.0299\n",
      "Epoch 90/100\n",
      "----------\n",
      "train Loss: 0.2977 Acc: 0.0285\n",
      "val Loss: 0.2910 Acc: 0.0294\n",
      "Epoch 91/100\n",
      "----------\n",
      "train Loss: 0.2972 Acc: 0.0284\n",
      "val Loss: 0.2868 Acc: 0.0306\n",
      "Epoch 92/100\n",
      "----------\n",
      "train Loss: 0.2976 Acc: 0.0284\n",
      "val Loss: 0.2875 Acc: 0.0306\n",
      "Epoch 93/100\n",
      "----------\n",
      "train Loss: 0.2978 Acc: 0.0283\n",
      "val Loss: 0.2871 Acc: 0.0304\n",
      "Epoch 94/100\n",
      "----------\n",
      "train Loss: 0.2968 Acc: 0.0288\n",
      "val Loss: 0.2901 Acc: 0.0299\n",
      "Epoch 95/100\n",
      "----------\n",
      "train Loss: 0.2964 Acc: 0.0287\n",
      "val Loss: 0.2882 Acc: 0.0301\n",
      "Epoch 96/100\n",
      "----------\n",
      "train Loss: 0.2953 Acc: 0.0293\n",
      "val Loss: 0.2873 Acc: 0.0302\n",
      "Epoch 97/100\n",
      "----------\n",
      "train Loss: 0.2965 Acc: 0.0286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2998 Acc: 0.0275\n",
      "Epoch 98/100\n",
      "----------\n",
      "train Loss: 0.2973 Acc: 0.0287\n",
      "val Loss: 0.2971 Acc: 0.0279\n",
      "Epoch 99/100\n",
      "----------\n",
      "train Loss: 0.2968 Acc: 0.0286\n",
      "val Loss: 0.2932 Acc: 0.0294\n",
      "Epoch 100/100\n",
      "----------\n",
      "train Loss: 0.2966 Acc: 0.0288\n",
      "val Loss: 0.3097 Acc: 0.0241\n",
      "Training complete in 570m 18s\n",
      "Best val Acc: 0.030959\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "model, hist = train_model(model, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.014014524143202955,\n",
       " 0.027646833991591284,\n",
       " 0.006242833481972226,\n",
       " 0.028220155433813225,\n",
       " 0.026818703019492927,\n",
       " 0.01847369091604026,\n",
       " 0.014205631290610268,\n",
       " 0.02478022678048159,\n",
       " 0.02974901261307173,\n",
       " 0.02962160784813352,\n",
       " 0.030003822142948146,\n",
       " 0.022104726716779207,\n",
       " 0.029685310230602623,\n",
       " 0.011466428844438782,\n",
       " 0.010956809784685948,\n",
       " 0.02917569117084979,\n",
       " 0.02872977449356606,\n",
       " 0.029685310230602623,\n",
       " 0.02261434577653204,\n",
       " 0.029366798318257104,\n",
       " 0.028347560198751433,\n",
       " 0.030577143585170087,\n",
       " 0.025863167282456363,\n",
       " 0.02962160784813352,\n",
       " 0.02331507198369219,\n",
       " 0.030831953115046502,\n",
       " 0.030704548350108295,\n",
       " 0.030258631672824565,\n",
       " 0.03019492929035546,\n",
       " 0.030386036437762772,\n",
       " 0.029876417378009938,\n",
       " 0.030449738820231876,\n",
       " 0.026882405401962035,\n",
       " 0.03032233405529367,\n",
       " 0.028538667346158748,\n",
       " 0.03064084596763919,\n",
       " 0.029303095935787997,\n",
       " 0.029557905465664416,\n",
       " 0.030003822142948146,\n",
       " 0.03032233405529367,\n",
       " 0.030831953115046502,\n",
       " 0.019811440947891452,\n",
       " 0.030386036437762772,\n",
       " 0.030386036437762772,\n",
       " 0.030003822142948146,\n",
       " 0.030577143585170087,\n",
       " 0.03051344120270098,\n",
       " 0.029430500700726208,\n",
       " 0.029239393553318893,\n",
       " 0.023697286278506816,\n",
       " 0.029940119760479042,\n",
       " 0.027646833991591284,\n",
       " 0.03064084596763919,\n",
       " 0.03095935787998471,\n",
       " 0.030386036437762772,\n",
       " 0.030704548350108295,\n",
       " 0.03006752452541725,\n",
       " 0.029366798318257104,\n",
       " 0.030831953115046502,\n",
       " 0.03051344120270098,\n",
       " 0.030577143585170087,\n",
       " 0.02872977449356606,\n",
       " 0.03095935787998471,\n",
       " 0.030577143585170087,\n",
       " 0.03064084596763919,\n",
       " 0.03051344120270098,\n",
       " 0.027774238756529495,\n",
       " 0.028984584023442478,\n",
       " 0.0307682507325774,\n",
       " 0.030386036437762772,\n",
       " 0.030577143585170087,\n",
       " 0.021786214804433685,\n",
       " 0.022932857688877564,\n",
       " 0.02726461969677666,\n",
       " 0.013186393171104599,\n",
       " 0.03006752452541725,\n",
       " 0.028538667346158748,\n",
       " 0.023697286278506816,\n",
       " 0.030577143585170087,\n",
       " 0.017772964708880113,\n",
       " 0.02904828640591158,\n",
       " 0.02509873869282711,\n",
       " 0.030449738820231876,\n",
       " 0.0307682507325774,\n",
       " 0.03064084596763919,\n",
       " 0.029303095935787997,\n",
       " 0.030449738820231876,\n",
       " 0.030704548350108295,\n",
       " 0.03064084596763919,\n",
       " 0.029940119760479042,\n",
       " 0.029430500700726208,\n",
       " 0.03064084596763919,\n",
       " 0.03064084596763919,\n",
       " 0.030386036437762772,\n",
       " 0.029876417378009938,\n",
       " 0.03006752452541725,\n",
       " 0.03019492929035546,\n",
       " 0.027455726844183972,\n",
       " 0.027901643521467703,\n",
       " 0.029430500700726208,\n",
       " 0.024143202955790546]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved Successfully\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"0_3lossmodel.pt\")\n",
    "print(\"Model Saved Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0801, -0.0972, -0.0801,  ...,  1.5810,  1.5982,  1.6153],\n",
       "          [-0.0458, -0.0629, -0.0458,  ...,  1.5810,  1.5982,  1.6153],\n",
       "          [ 0.0398,  0.0227,  0.0227,  ...,  1.5982,  1.6153,  1.6324],\n",
       "          ...,\n",
       "          [-1.5699, -1.4158, -1.4672,  ...,  1.2557,  1.1187,  0.9988],\n",
       "          [-1.5870, -1.5870, -1.5870,  ...,  1.3242,  1.1872,  1.0673],\n",
       "          [-1.5870, -1.5870, -1.5870,  ...,  1.3927,  1.2385,  1.1187]],\n",
       " \n",
       "         [[-0.0224, -0.0399, -0.0399,  ...,  1.8158,  1.8333,  1.8508],\n",
       "          [ 0.0126, -0.0049, -0.0049,  ...,  1.8158,  1.8333,  1.8508],\n",
       "          [ 0.1001,  0.0826,  0.0826,  ...,  1.8333,  1.8508,  1.8683],\n",
       "          ...,\n",
       "          [-1.4930, -1.3354, -1.3880,  ...,  1.6057,  1.4832,  1.3782],\n",
       "          [-1.5105, -1.5105, -1.5105,  ...,  1.6758,  1.5532,  1.4482],\n",
       "          [-1.5105, -1.5105, -1.5105,  ...,  1.7458,  1.6232,  1.5007]],\n",
       " \n",
       "         [[-0.0092, -0.0267, -0.0267,  ...,  2.0474,  2.0648,  2.0823],\n",
       "          [ 0.0256,  0.0082,  0.0082,  ...,  2.0474,  2.0648,  2.0823],\n",
       "          [ 0.1128,  0.0953,  0.0779,  ...,  2.0648,  2.0823,  2.0997],\n",
       "          ...,\n",
       "          [-1.3339, -1.1770, -1.2293,  ...,  1.6814,  1.5420,  1.4374],\n",
       "          [-1.3513, -1.3513, -1.3513,  ...,  1.7511,  1.6117,  1.4897],\n",
       "          [-1.3513, -1.3513, -1.3513,  ...,  1.8034,  1.6640,  1.5245]]]),\n",
       " tensor([27.7299]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img,lab=train_dataset.__getitem__(0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24.9208]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(img.unsqueeze(0).to('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([27.7299])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
